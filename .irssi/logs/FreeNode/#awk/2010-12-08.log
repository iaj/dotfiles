--- Log opened Wed Dec 08 00:00:50 2010
00:40  paissad: hello,
00:40  paissad: awk '{print $NF}' prints the number of fields ... but i would like to print directly the last field, is it possible ?
00:40  cthuluh: well
00:41  paissad: i know i could put that returned value (NF) into a variable and then do -> awk '{prtin \$$var}' 
00:41  cthuluh: here, this does print the last field
00:41  cthuluh: with the three awk versions I have
00:42  paissad: my God, sorry ,i'm really silly , really
00:42  paissad: i forgot the '$' before NF
00:42  paissad: damn it
00:42  cthuluh: ;)
00:42  cthuluh: you're going nowhere with print \$$var, though
00:43  Riviera: actually
00:43  Riviera: $($NF) should work
00:43  Riviera: no idea about $$NF though.
00:43  Riviera: ah, $$NF works indeed
00:43  Riviera: # awk '{ print $$NF }' <<< 'one two three 2'
00:44  Riviera: 4# awk '{ print $$NF }' <<< 'one two three 2'
00:44  shbot: Riviera: two
00:45  cthuluh: haha
00:46  waldner: 4# awk '{print $$$NF }' <<< 'one 1 three 2'
00:46  shbot: waldner: one
00:46  waldner: cool
00:46  ket: wtf..
00:46  waldner: one coulld walk the entire record if fields are all numeric
00:47  waldner: 4# awk '{print $$$$NF }' <<< '4 1 2 3'
00:48  shbot: waldner: 4
00:48  waldner: (all but the very last)
00:49  waldner: 4# awk '{print $$$$$$$$$$NF }' <<< 'this 1 2 3 4 5 6 7 8 9'
00:49  shbot: waldner: this
00:49  gnomon: Interesting exercise: write a permutation walker with a series of nested if's of increasing $ depth.
00:49  gnomon: It's easy and fun for the whole family  :)
00:51  ket: so $$ is nothing in awk?
00:52  waldner: it's just a further level of indirection
00:55  ket: does adding "$" to $NF infinitely subtract NF by one?
00:55  Riviera: ket: no
00:55  Riviera: ket: $$NF is just $($NF)
00:56  Riviera: ket: like $a is the value of field 10 if a is 10,
00:56  Riviera: ket: $$NF is the value of 10 if the last field of the current record is 10, like for the line "a b 10"
00:57  Riviera: s/value of 10/value of field 10/
00:57  Riviera: ket: because $NF is the value of the last field.
00:57  ket: ok..
00:57  Riviera: ket: your ".." makes me suspicious
00:57  Riviera: ket: really ok? :)
00:57  ket: ok..
00:58  ket: not really.. cause i wanted to suggest $$$NF as $($($NF))
00:58  Riviera: but that's what it is
00:58  ket: then yea.. i see..
01:01  ket: i should probably stop playing with that...
01:01 * ket -- carries on...
01:02  Riviera: :)
01:02  ket: ;)
--- Log closed Wed Dec 08 01:48:25 2010
--- Log opened Wed Dec 08 02:00:04 2010
02:00 --- Users 99 nicks [0 ops, 0 halfops, 0 voices, 99 normal]
02:02 --- Channel #awk was synced in 146 seconds
--- Log closed Wed Dec 08 02:26:18 2010
--- Log opened Wed Dec 08 02:26:43 2010
02:26 --- Users 100 nicks [0 ops, 0 halfops, 0 voices, 100 normal]
02:29 --- Channel #awk was synced in 147 seconds
02:37  g0pher: off topic: any advantage formating my 4GB SDHC card to exFAT ?
02:39  gnomon: g0pher, you'll be able to fill it with a single 4GiB file, I suppose.
02:40  g0pher: just use for backup eg about 4~8 MB zip files
02:41  gnomon: It's possible that the exFAT system may be more resilient to the kind of data corruption which often crops up in removable media..?
02:41  gnomon: I don't know, I'm grasping at straws.  I dislike exFAT for political reasons and can't really see why it's superior to any of the other filesystems already in the niche that it's trying to fill.
02:42  gnomon: But I'm a grumpy curmudgeon, so you should ignore everything I say.
02:45  steve___: ...and a giant awk tease!  ;)
02:47  gnomon: Bah, I'll get around to posting that stuff at some point.  I'm in the process of recovering from a /home/ disk crash and digging through my backups isn't at the top of my priority list.
02:47  gnomon: ...which, if you look at it sideways, sounds a lot like "I'm sorry and I'll get to it soon"  :)
02:47  gnomon: (it's really nothing phenomenal.  Don't set your expectations too high)
02:49  g0pher: yes - exFAT adds ability to use over 4GB file - which is not an issue with 4GB device
02:50  steve___: gnomon: s'all good.  You piqued my keen.  No worries.
--- Log closed Wed Dec 08 02:55:00 2010
--- Log opened Wed Dec 08 02:55:22 2010
02:55 --- Users 102 nicks [0 ops, 0 halfops, 0 voices, 102 normal]
02:57 --- Channel #awk was synced in 145 seconds
02:59  g0pher: what would be best sector size - came formatted FAT32  512 byte
03:03  g0pher: chkdsk says: `32,768 bytes in each allocation unit`  -- does that mean the cluster size is 32k ?
--- Log closed Wed Dec 08 08:55:23 2010
--- Log opened Wed Dec 08 09:05:48 2010
09:05 --- Users 100 nicks [0 ops, 0 halfops, 0 voices, 100 normal]
09:08 --- Channel #awk was synced in 146 seconds
--- Log opened Wed Dec 08 10:03:38 2010
10:03 --- Users 99 nicks [0 ops, 0 halfops, 0 voices, 99 normal]
10:05 --- Channel #awk was synced in 99 seconds
13:30  ptl: Is awk awkward?
13:31  pgas: a bit, but not too much
13:33  ptl: more than sed? or less?
13:33  pgas: far less
13:34  ptl: I mean more or less than sed, not more than sed and more than less. :P
13:34  ptl: bad pun...
13:35  ptl: I was just joking. I love awk. It "saved" me too many times. But I still struggle with sed, I confess.
13:35  pgas: ah
16:22  pr3d4t0r: Good morning.
16:25  pgas: morning to you
16:45  pr3d4t0r: pgas: :D
16:45  pr3d4t0r: pgas: I've been play0ring, sort of, with sazl.
16:46  pr3d4t0r: pgas: awk for binary files, from Rob Pike at Google.  Sort of.
16:50  steve___: pr3d4t0r: sazl?  Do you have a url?
16:54  pgas: pr3d4t0r: yeah I saw you saying as much the other day...I'm more on the embed side of linux ;)
16:58  pr3d4t0r: pgas: :)
16:58  pr3d4t0r: steve___: Google.
16:58  gionnico: hi
16:59  pr3d4t0r: steve___: I google it every time I want to look at their pages.
16:59  gionnico: how can I remove all initial spaces before a character?
16:59  pgas: before what character?
16:59  gionnico: pgas: 0-9aZ
17:00  pgas: but at the beginning of the line? everychar? or what?
17:01  gionnico: at beginning of every line the file have 1 2 3 .. n spaces before text
17:01  pgas: give us a line of input and one of the expected output
17:01  pgas: sub(/^ +/,"")
17:02  steve___: pr3d4t0r: I did this.  The top result for 'sazl' is 'Sazl - Road Engineering, Mapping and GIS Resources'
17:02  gionnico: pgas: http://pastebin.ca/2013891
17:04  gionnico: awk '{sub(/^ +/,"")}' file gives empty output
17:05  pgas: you need to print the line
17:05  gionnico: ah! i see w/o {} is also working
17:05  pgas: awk '{sub(/^ +/,"")}1' #for instance
17:07  gionnico: thanks
17:12  pr3d4t0r: steve___: heh.
17:13  pr3d4t0r: steve___: Did you find it yet?  If not I'll work my way back from the sources.  It's on Google Code.
17:13  pr3d4t0r: steve___: I'm distracted with work, hence my latency.
17:15  pr3d4t0r: steve___: http://code.google.com/p/szl/
17:16  pr3d4t0r: steve___: I had to find Rob's page first, then a paper, then google for "Sawzall", then from Wikipedia find the link I just gave you :)
17:22  gionnico: here's something more difficult i explained in this pastebin: http://pastebin.ca/2013912
17:22  gionnico: maybe too difficult for only awk or for a one liner ..
17:22  steve___: pr3d4t0r: cool, thank you.
17:23  gionnico: err http://pastebin.ca/2013914
17:32  go|dfish: gionnico: awk '{ sub(/:[^:]+$/,"",$1); totals[$1]+=$2; } END{ for (t in totals) print t, totals[t] }'
17:39  gionnico: go|dfish: seems to have some problems when changing hour ..like with this http://pastebin.ca/2013931
17:41  gionnico: go|dfish: it's sorting problem really, computations seem correct
17:42  gionnico: go|dfish: but " | sort " seems also work thanks a lot
17:42  go|dfish: ah, that
17:42  go|dfish: hmmm
17:44  go|dfish: perhaps it's only gawk that you can change that behaviour with, i forget
17:44  go|dfish: i seem to recall some setting to make it not do that
17:45  go|dfish: ah yes, The order that var transverses the indices of array is not defined.
17:45  go|dfish: it has been a while 
18:48  zxvff: hi!
18:49  zxvff: sort -gr node01.du| awk '$1 > 256000 {print $2}' <--- I want to take each $2 and look for it in file node02.last, is there an easy way to do this?
18:56  waldner: and once you find them in the second file, what do you want to do?
18:57  waldner: (or not find then)
18:58  zxvff: print it i suppose
18:58  zxvff: i'm still figuring that out
18:59  zxvff: actually when i find the second file (output from the last(1) ) i need to only print it if $2 (username) has not been logged in in a week
18:59  zxvff: i probably haven't used awk in 2 years :(
19:06 --- WinstonSmith_ is now known as WinstonSmith
19:11  waldner: ok, so what's a sample $2 from the command above?
19:11  zxvff: just supposed to be a username, but they are hashed, so.... ef399b2d is one
19:12  zxvff: waldner: http://genoci.de/upload/README <--- that's what i'm trying to accomplish
19:13  waldner: homework, eh?
19:13  zxvff: nah it's for a job interview :)
19:13  waldner: well, similar
19:13  zxvff: true
19:14  zxvff: my scripting isn't as strong as it should be i suppose, but i haven't held a sysadmin position in a couple years
19:15  waldner: but ok, seems doable
19:15  zxvff: could probably do it in python but seems like i could make shorter work of it in awk
19:18  waldner: at least the interviewer looks serious
19:18  waldner: unlike many I've seen
19:20  waldner: it's not going to be short, though
19:20  zxvff: :(
19:20  zxvff: think i'm better off attacking it with python?
19:21  waldner: probably
19:21  waldner: or perl
19:21  zxvff: :(
19:21  waldner: if nothing else because of their built-in capabilities and modules available
19:21  waldner: I seem to see there's also some amount of date arithmetic and conversion
19:22  waldner: well, gawk has probably useful for that as well
19:22  waldner: s/y/y something/
19:23  waldner: how do they define "machines they have not logged into recently
19:23  waldner: ?
19:23  zxvff: they don't, it's supposed to be open ended
19:23  waldner: ah ok, so one can just set a variable for that
19:23  zxvff: as much of a "how do you approach the problem" thing as a "do you know what the fuck you are doing when you are scripting" thing
19:23  waldner: yes
19:23  zxvff: i was just gonna call it a week
19:24  zxvff: since i only have 30 days worth of data
19:24  waldner: if you convert everything in days, or some other unit, then you can just pass that period to the code
19:25  waldner: just out of curiosity, which role is this interview for?
19:25  zxvff: sysadmin
19:26  waldner: ok, so
19:26  zxvff: i can probably use the awk mktime function
19:26  zxvff: to find the date difference
19:26  waldner: one option is to read the whole thing in memory, then do the necessary calculations
19:26  waldner: how many files are ther? thousands?
19:27  zxvff: nah just 8
19:27  zxvff: 9
19:27  waldner: 9 .last and 9.du?
19:27  zxvff: yeah
19:27  waldner: ok so reading them in memory is perfectly doable
19:28  waldner: so we could write code like
19:28  waldner: awk '.... main code ... END { ..code to print results... }' *.last *.du
19:30  waldner: assuming no empty files, we can start with
19:30  zxvff: considering i need to do something specific for each file (because hostname is only viewable in the filename) i was thinking something more like for x in `ls | grep *.du` { echo node01.du | awk ' ... ' }
19:30  waldner: well if you do that, then just do for x in *.du
19:31  zxvff: which is the more logical way to approach it?
19:31  waldner: you need data taken from all the files
19:31  waldner: so you want to collect all the data before you can calculate anything
19:32  waldner: in awk, we can take the machine name with code like this:
19:32  waldner: NR==1{machine = substr(FILENAME, 1, index(FILENAME,".")))}
19:32  waldner: assuming there is only a dot
19:32  zxvff: correct assumption
19:33  waldner: in that same block, we can also recognize what kind of file it is
19:33  waldner: we could have a variable, eg "type", which is 1 for a last file, and 2 for a du file
19:33  waldner: so eg
19:33  tyarusso: I have an integer of variable length, but always longer than 10 characters.  How could I drop the last 10 (least significant) characters and keep whatever the rest is?
19:33  zxvff: you've got an extra ) though :)
19:34  waldner: NR==1{machine = substr(FILENAME, 1, index(FILENAME,"."))); type = FILENAME ~ /last$/?1:2}
19:34  waldner: yes
19:34  waldner: then, line 1 must always be skipped, because it's a heading line
19:34  waldner: so to complete that
19:34  waldner: tyarusso: sub(/.{10}$/,"", number)
19:35  waldner: NR==1{machine = substr(FILENAME, 1, index(FILENAME,".")); type = FILENAME ~ /last$/?1:2; next}
19:35  waldner: now for all the other lines, you should parse differently depending on whether this is a last or du file (ie, the value of "type")
19:36  waldner: so you'd do something like
19:36  waldner: NR==1{see above} type == 1 { this is a line from a last file } type == 2 { this is a du file }
19:37  tyarusso: waldner: I'm actually piping this value in from a shell command - what would the structure look like in that context?
19:37  waldner: you can also say type == "last" and type == "du" if you prefer, it's more readable
19:37  zxvff:  | awk 'sub(/.{10}$/,"", number) ???
19:38  zxvff: just pipe it?
19:38  waldner: tyarusso: it's difficult without seeing the code
19:38  waldner: what command are you running?
19:38  tyarusso: waldner: echo "1152921504606846976" | awk ....something
19:39  waldner: yes, that "something" is what I wrote above
19:39  waldner: put it in signle quotes
19:39  waldner: echo ..... | awk '{sub(/.{10}$/,""); print}'
19:39  waldner: if it doesn't work, try with awk --re-interval
19:40  waldner: if that doesn't work either, you have to do sub(/..........$/,"")
19:40  zxvff: waldner: i guess i'm confused about the structure of this script as you're proposing it, becuase I don't know what i'd do with type == last, because the .last files don't matter until after i've pulled the data out of the .du file
19:40  waldner: they don't, but you have to have that data somewhere once you've parsed the du files
19:40  tyarusso: waldner: With --re-interval it works.
19:40  zxvff: ie "sort -gr node01.du| awk '$1 > 256000 {print $2}'" gives me all the info i need from the .du file, and once i have that i only ned to worry about the .last file
19:41  waldner: does it?
19:41  waldner: how do you get the info about which machines have the more space?
19:41  waldner: *most
19:41  zxvff: well i'm just worrying about the first task right now :)
19:41  waldner: ok
19:41  tyarusso: waldner: is that a GNU extension though, or standard?
19:41  waldner: do you want to write three different programs for the three tasks?
19:42  zxvff: i figured i'd script each part separately and then tie them together in a bash script
19:42  zxvff: waldner: yeah i'll probably do that
19:42  waldner: tyarusso: it's standard (although gnu awk requires --re-interval to enable it; this will change in the future and become the default)
19:42  waldner: zxvff: ok my approach was to do all in a single program
19:43  waldner: which can be easily modified if a new requirement arises
19:43  waldner: with your approach, you have to writ a brand new code
19:43  waldner: which may contain some duplicated code from another solution
19:43  waldner: but ok
19:43  waldner: the important point is: whatever you choose, you must be able to justify it
19:43  waldner: that's what they want to see
19:44  waldner: there's no signle right answer, it depends on many things
19:44  waldner: so let's go with the "one task at a time" approach
19:46  waldner: so first of all, you have to do awk 'NR>1 && $1 > 256000 etc.
19:46  waldner: you don't want to process line 1
19:46  waldner: (I'm still not convinced this is a good way to go, but let's see)
19:47  waldner: and, I'd say $1 > 2000, since that's what they are asking for
19:47  waldner: where did you get the 256000 from?
19:47  zxvff: du is in bytes, there are 256000 bytes in 2000KB
19:48  waldner: yes, but the column is "sizeK"
19:48  waldner: and they want people with more than 2000k
19:48  waldner: (and no, 2000KB is not 256000 bytes, sorry)
19:49  zxvff: wait
19:49  zxvff: 2000 kilobytes = 2 048 000 bytes
19:49  zxvff: that's what i typed into google
19:49  zxvff: originally
19:49  zxvff: and thought i used the google number
19:49  zxvff: lol :/
19:49  zxvff: also the files they give me don't have column names in them
19:50  waldner: ah ok
19:50  zxvff: so we don't need to skip line 1, but also that's how i missed that it was sizeK
19:50  waldner: right
19:50  waldner: but:
19:51  waldner: so far, you are just printing a list of users
19:51  waldner: how do you know which is the machine where they are using that much space?
19:51  waldner: user a may be using 2500k on one machine, 3000k on another machine, 10k on another
19:51  waldner: your code will just print the username twice
19:51  zxvff: well i was going to write a loop that would do that, and use the filename as a variable
19:52  waldner: awk can already loop over files
19:52  waldner: why not output (user,machine) pairs then?
19:53  waldner: awk 'NR==1{machine = substr(FILENAME, 1, index(FILENAME,"."))} $1 > 2000{print $2, machine}' *.du
19:53  zxvff: i was going to do $1.du {get usernames over 2MB} | cat $1.last | grep $username | {find last login} | if lastLoginLessThan1Week {print $username} else {do nothing}
19:53  zxvff: :/
19:53  waldner: you can do it with awk only
19:53  waldner: probably
19:54  zxvff: that would be better
19:54  waldner: so we're printing (user,machine) for users with high usage
19:54  waldner: that can be the input for another awk
19:55  zxvff: right
19:55  waldner: (hopefully at some point you'll see why I wanted to do it all with a single awk)
19:56  waldner: now this second awk will first read all the last files
19:56  waldner: to track the last login of a user on a given machine
19:57  waldner: then it will read the stdin, and find out whether a user last logged into the associated machine "not recently"
19:57  waldner: (it's not ok yet, because then you'll want to print UNIQUE usernames, I think)
19:58  zxvff: the first one only grabs the node01. files
19:58  zxvff: so it isn't iterating through all the files
19:58  waldner: it's inefficient that way
19:58  waldner: what if you have
19:58  waldner: user_a node01
19:58  waldner: user_b node01
19:58  waldner: ...
19:58  waldner: user_k node01
19:58  waldner: you'd end up reading node01.last a lot of times
19:59  zxvff: ah, true
19:59  waldner: so, forst read all .last files once and store the info in memory, then it's easy to compare stdin with that info
19:59  waldner: *first
20:00  waldner: now parsing the last file may be tricky
20:00  waldner: it's in reverse cronological order (most recent first)
20:01  waldner: basically, the idea is that, within a single .last file, you only care about the FIRST line you find for a given user
20:01  waldner: that is his last login
20:01  zxvff: right
20:02  waldner: so we can keep a hash (== associative array)
20:02  waldner: whose key is (user,machine)
20:03  waldner: and the value is the time of last login (in seconds, for example)
20:03  waldner: so it would be like login[user_a,node01] = 12847748778
20:03  waldner: and there will be meny such elements
20:04  waldner: as you read each file, you have to check if (user,machine) already exists as a key or not
20:04  waldner: if not, you parse the login time, and create the element
20:04  waldner: if it exists already, just ignore it
20:04  waldner: so the skeleton code will be:
20:05  waldner: awk 'NR==1{extract machine name} !(($1,machine) in login) { parse login time and add it to the array}' *.last
20:06  waldner: to extract the machine name, it's still taken from the file name, so 'NR==1{machine = substr(FILENAME, 1, index(FILENAME,"."))} etc.
20:06  waldner: now for the second part
20:07  waldner: the login time is actually four fields:
20:07  waldner: the date is like "Tue Nov 30", then there's the time
20:07  waldner: you have to take $4, $5, $6 and $7
20:08  waldner: and convert it to seconds
20:09  waldner: now, gawk's mktime() wants a string like YYYY MM DD HH MM SS
20:09  waldner: so we need to mess around a bit
20:09  waldner: YYYY, we can assume 2010
20:09  waldner: DD, we can use $6
20:10  waldner: then we have to split $7 into HH and MM
20:10  waldner: convert $5 into the month number
20:10  waldner: and again assume 00 for SS
20:11  waldner: right?
20:11  zxvff: right
20:11  zxvff: that makes sense but you're way ahead of me so far :)
20:11  waldner: so we can set up a commodity array indexed by month name to do the conversion
20:11  waldner: we do that in a BEGIN block
20:12  waldner: BEGIN{m["Jan"]=1;m["Feb"]=2; etc.etc.}
20:12  waldner: we can assume the time is always HH:MM, so we can just take the first two characters and the last two of $7
20:13  waldner: so to sum up:
20:15  waldner: if (!($1,machine) in login) { login[$1,machine] = mktime( "2010 " m[$5] " " $6 " " substr($7,1,2) " " substr($7,4,2) " 00")}'
20:15  waldner: and we read all the .last files like that
20:16  waldner: then, to see how far in the past the login is, we need a timestamp for "now" or "today"
20:16  waldner: we can calculate that at the beginning, in the same BEGIN block
20:17  waldner: eg, today = systime()
20:17  waldner: (better: now = systime)
20:17  waldner: again you may choose any time, as long as you can provide a reason for your choice
20:18  waldner: then we need something to define what "recently" means
20:18  waldner: for example, to keep it simple we can assume it means "1 week"; best is if you make it a variable that the user passes in, which is fairly easy with awk
20:19  waldner: you say awk -v recent=7 '...' to mean 7 days
20:19  waldner: and then you use "recent" in the script, so it can be passed different values
20:20  waldner: again, in the BEGIN block you can convert that in seconds (easy: recent = recent * 86400)
20:20  waldner: now we're ready to go on, and read the input tuples from the awk we ran previously
20:21  waldner: the overall structure of the script has to be modified a bit 
20:21  waldner: so you now have
20:24  waldner: awk 'NR==1{machine = substr(FILENAME, 1, index(FILENAME,"."))} $1 > 2000{print $2, machine}' *.du | awk -v recent=7 'BEGIN{now=systime();recent=recent * 86400; m["Jan"]=1;...} NR==FNR && NR==1{machine = substr(FILENAME, 1, index(FILENAME,"."))}NR==FNR{if(!($1,machine) in login) { login[$1,machine] = mktime( "2010 " m[$5] " " $6 " " substr($7,1,2) " " substr($7,4,2) " 00"); next} now - login[$1,$2] > recent { #we found an interesting user }' *.last
20:24  waldner: the added part is at the end, the ... now - login[$1,$2] {...}
20:25  waldner: er, now - login[$1,$2] > recent {...}
20:25  waldner: that checks to see whether the last login time of the (user,machine) coming from the input happened more than "recent" seconds ago (as calculated previously)
20:26  waldner: (I told you this wasn't going to be short)
20:26  zxvff: oh god that hurts my head
20:26  waldner: it's all basic awk
20:27  waldner: there's nothing esoteric, provided you know the basics of awk and can read the docs for the relevant functions
20:27  waldner: let's focus on what whould go into that last block
20:28  waldner: since we've found a user whose login on the machine he has high usage is "not recent", we have to do something
20:29  waldner: the description isn't really detailed, so you may just choose to print the tuple
20:29  waldner: eg, print $1, $2
20:29  waldner: which I think is sensible, because it gives the most information
20:29  waldner: it's not enough to know the user, but you also have the info about the machine
20:30  waldner: I can paste a better formatted version of the above code on pastebin, so you can study it
20:30  zxvff: yes please, i'm in the middle of trying to dissect it and break it into parts
20:30  zxvff: for readability
20:31  zxvff: also, thank you for taking such a long time out of your day to offer me some incredible help :)
20:33  waldner: http://pastebin.com/raw.php?i=yXVF9aAd
20:34  skered: is there a special var for how many files you've processed?
20:34  waldner: GNU awk has ARGIND
20:35  waldner: otherwise, you can do NR==1{count++} etc.
20:35  waldner: with the caveat that it will be skipped for empty files
20:35  waldner: zxvff: the NR==FNR is an awk idiom used when processing two files
20:35  waldner: (usually)
20:36  waldner: the basic pattern is awk 'NR==FNR{process first file; next} { process second file }' file1 file2
20:36  waldner: ah wait, I just see that I'm doing it utterly wrong
20:36  waldner: lol
20:36  waldner: there are multiple .last files
20:36  zxvff: yes
20:37  waldner: it needs a slight change
20:37  waldner: http://pastebin.com/raw.php?i=bsSkg5W2
20:38  waldner: so the FILENAME ~ /last$/ is true all the time while processing the .last files
20:38  skered: ARGV[1] == FILENAME { do stuff for file1 } ARGV[2] == FILENAME { do stuff for file2 }
20:38  tag: That's strange, why not just use 'FILENAME == ARGV[2] { /* File 1 */ } FILENAME == ARGV[3] { /* File 2.. */ }' etc...?  it would actually work
20:38  tag: yeah skered I was just thinking the same thing
20:38  waldner: yes that's another way to do it indeed
20:39  waldner: and mosr euseful when there are > 2 files
20:39  waldner: *most
20:40  waldner: zxvff: that is still using two awk invocations
20:40  zxvff: right
20:40  waldner: unifying them so that all the .du and .last files are processed at once is just some keystroks away
20:40  waldner: (which is what I wanted to do initially, and also puts you in a better position to solve the other tasks in one fell swoop)
20:41  zxvff: and that is making more sense to me conceptually now
20:41  skered: tag: ARGV[1] is file1...
20:41  skered: ?
20:46  waldner: zxvff: actually there was another error: http://pastebin.com/raw.php?i=gXDjUDC0
20:46  waldner: (line 1 changed NR into FNR)
20:48  zxvff: i get a syntax error at "if(!($1,machine)"
20:48  zxvff: every time
20:49  skered: awk 'exp && var = $1 { ... ' you can perform assignments in a patten?
20:49  skered: pattern 
20:49  waldner: skered: yes, although some may consider it not very readable
20:49  waldner: zxvff: let me see
20:50  waldner: ah yes, misses a bracket
20:50  waldner: no it's fine
20:50  waldner: let me try to run it
20:50  skered: er I forgot with you can name an array by using a var...
20:52  waldner: works for me, although I discovered other minor issues
20:52  zxvff: weird, let me try it on my other system
20:52  skered: or can you? awk '{ $1[$2]
20:53  skered: ... }
20:55  waldner: http://pastebin.com/raw.php?i=Y1U0dGzZ
20:55  waldner: skered: no you can't
20:55  waldner: for that you'd need something like eval, but awk doesn't have it
20:56  skered: Yeah, for some reason I thought it would allow that.. I guess I'm just so use to use positional vars as keys to an array the same thought extended to array names.
21:29  skered: How much of a hack is this?
21:30  skered: awk -F, 'ARGV[1] == FILENAME { c[$1$2$3]++ } ARGV[2] == FILENAME && ( ( i=$3$1$4 && c[$3$1$4] ) || ( i=$5$1$4 && c[$3$1$4] ) ) { print c[i] } '
21:31  skered: Load file1 into an array, while processing file2 check to see if $3$1$4 or $5$1$4 is loaded value from file1?
21:32  zxvff: waldner: I still can't make that one line work without throwing an error :( 
21:32  zxvff: if(!($1,machine) in login){
21:32  waldner: zxvff: paste the actual command that you're running (on pastebin)
21:32  zxvff: here is the error:
21:32  zxvff: awk: cmd. line:9:   if(!($1,machine) in login){ 
21:32  zxvff: awk: cmd. line:9:          ^ syntax error
21:32  zxvff: i'll paste
21:33  waldner: skered: you can remove the ARGV[2] == FILENAME art if you put a ;next at the end of the first block
21:33  waldner: zxvff: you must run it gawk
21:33  zxvff: ah
21:34  zxvff: i thought that awk == gawk on most systems
21:34  waldner: and yet, it works for me in gawk, mawk and busybox
21:34  waldner: which system are you on?
21:34  zxvff: yeah, still same error
21:34  zxvff: i tried it on my os x machine and a debian one
21:34  zxvff: want a shell on the system i'm trying to run it on? :)
21:34  waldner: try on the debian one, but install gawk first
21:34  zxvff: it has gawk
21:35  waldner: are you sure?
21:35  waldner: what does awk --version say?
21:35  zxvff: gawk is already the newest version.
21:35  waldner: debian has an old and broken mawk by default
21:35  waldner: yes, but is the "awk" you run pointing to gawk?
21:35  zxvff: zxvf@RUMMEX:~/awkawkawk/playground-usage-reports/datafiles$ awk --version
21:35  zxvff: GNU Awk 3.1.5
21:35  waldner: ok
21:35  waldner: yes, give me a shell on that machine :)
21:36  waldner: I thought if (a,b) in array should be fairly standard
21:36  waldner: 3.1.5 is a bit buggy ISTR
22:11  skered: Is there an issue with using the assigment in the pattern with my above example?
22:13  waldner: AFAICT, it should work
22:17  kotique: hey, can awk return exit status like grep -q ?
22:18  waldner: it can
22:18  waldner: use "exit n" where n is the exit status you want
22:18  kotique: sed /first/,/last=\d/{/last=0/p}
22:18  kotique: now I want this in awk :)
22:30  kotique: thanks, no need
23:37  maj: i am trying to filter out any lines which field 2 starts with *
23:37  maj: awk -F ":" '{if($2!="*") print $1 "       \t\t" $5}' /etc/passwd
23:38  maj: i tried "^*" but that doesn't work
23:46  maj: any suggestions?
23:59  cthuluh: awk -F : '$2 ~ /^*/ { print $1 "..." $5; }'
--- Log closed Thu Dec 09 00:00:18 2010
