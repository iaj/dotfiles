--- Log opened Mon Dec 20 00:00:07 2010
00:00 --- Users 95 nicks [1 ops, 0 halfops, 0 voices, 94 normal]
00:01 --- Channel #awk was synced in 141 seconds
00:11  galaxywatcher: I am curious what an awk solution to finding the longest word in /usr/share/dict/words would look like..
00:12  ferret: it would be a reasonably boring state machine like in any other language
00:12  ferret: It's impossible to do without inspecting every line of the file at least once, and it can be done inspecting every line of the file only once
00:13  galaxywatcher: ferret: That's ok.
00:13  g0pher: {for(i=1; i<=NF; i++) if(maxword < length($i)) maxword=length($i);}  END{print maxword}
00:14  galaxywatcher: g0pher: That printed "24" 
00:14  galaxywatcher: What is the actual word?
00:15  g0pher: {for(i=1; i<=NF; i++) if(maxword < length($i)) {maxword=length($i); word=$NF}}  END{print maxword, word}
00:15  ferret: g0pher: why would you need a loop?
00:15  galaxywatcher: And in my /dict, it's a 5 way tie for 23 chars each.
00:15  ferret: fwiw, that file contains no spaces
00:16  galaxywatcher: formaldehydesulphoxylate is the winner :)
00:16  galaxywatcher: thanks g0pher 
--- Log closed Mon Dec 20 00:25:15 2010
--- Log opened Mon Dec 20 00:30:39 2010
00:30 --- Users 95 nicks [1 ops, 0 halfops, 0 voices, 94 normal]
00:32 --- Channel #awk was synced in 126 seconds
00:34  g0pher: how done without loop ? and what is the deliminator ?
--- Log closed Mon Dec 20 01:24:42 2010
--- Log opened Mon Dec 20 01:30:11 2010
01:30 --- Users 93 nicks [1 ops, 0 halfops, 0 voices, 92 normal]
01:31 --- Channel #awk was synced in 114 seconds
--- Log closed Mon Dec 20 03:25:01 2010
--- Log opened Mon Dec 20 03:30:28 2010
03:30 --- Users 92 nicks [1 ops, 0 halfops, 0 voices, 91 normal]
03:32 --- Channel #awk was synced in 110 seconds
04:15 --- luptenschteiner is now known as probonono
05:24  galaxywatcher: I have a small csv file I want to process. File has 2 fields like so: apple,23 \n apple,31 \n orange,5 \n orange,17, etc. I want to output the sum of the values for each unique Field 1. So output would be: apple,54 \n orange,22. etc.. I can do this in a multi-step process, but an awk one-liner would be great. Thanks.
05:26  galaxywatcher: I know I can sum a column of numbers: awk '{sum += $1} END {print sum}' # but how do I tie this all together?
05:28  galaxywatcher: awk -F"," '/apple/ {sum += $2} END {print sum}' # will work for apple...But I want to have an arbitrary number of $1 to 'uniq-ify'
05:35  j_wright: so just make an array
05:35  probonono: galaxywatcher, just use an array?
05:41  j_wright: remembering that arrays are really associative arrays
05:42  galaxywatcher: can you provide a reference, link or helpful hint?
05:45  j_wright: sum[ $1 ] += $2
05:46  j_wright: then look at 'for'
05:47  galaxywatcher: That's what I am reading now..This will take me a while..Thanks./
05:50  j_wright: i can tell you the answer if you really need it
05:54  galaxywatcher: I would appreciate it...And it would be more instructive than googling for awk tutorials.
05:58  j_wright: awk -F"," '{sum[ $1 ] += $2} END { for (key in sum) print key "=" sum[ key ]}'
06:01  galaxywatcher: j_wright: Thanks. It looks good.
06:07  galaxywatcher: j_wright: I don't understand why "sum[ $1 ]" is used. $1 is a variable not an integer.
06:07  galaxywatcher: string rather
06:08  j_wright: arrays in awk are like dictionaries in python, hashes in perl
06:09  j_wright: they are associative arrays, based on keys
06:09  j_wright: which can be numbers or strings
06:13  galaxywatcher: I understand that. So breaking this down a bit, when one line gets processed, say apple,31 we have {sum[apple] += 31} then apple,12 the same line will then equate to {sum[apple] += 43}, right?
06:16  j_wright: sum[apple] += 12
06:17  j_wright: or sum[apple] = sum[apple] + 12
06:17  galaxywatcher: The entire file gets processed first. When that ENDs, the for loop print the keys.
06:18  galaxywatcher: key value pairs.
06:18  j_wright: yes, for each key in the sum array it prints the name and the value
06:18  galaxywatcher: ok
06:19  galaxywatcher: sum[apple] stores the running total for apple. sum[orange] stores the running total for orange, right?
06:20  j_wright: yes
06:20  galaxywatcher: ok
06:20  galaxywatcher: THanks..
06:20  j_wright: np
07:01  skered: Why wouldn't awk -v RS="[[:space:]]" .. not read each word as $)?
07:01  skered: $0 rather
--- Log closed Mon Dec 20 08:31:21 2010
--- Log opened Mon Dec 20 09:19:45 2010
09:19 --- Users 92 nicks [1 ops, 0 halfops, 0 voices, 91 normal]
09:21 --- Channel #awk was synced in 114 seconds
16:09  xelister: hi, how to count how many lines in subdirs of foo/  contain text  $something,  where something !='html' and !='form'.  So line "$x" counts, line "$html" does NOT count, and "$html $foo" counts again.
16:10  gnubien: find might be able to do that
16:11  xelister: Im tyring egrep
16:11  xelister: but how to say in egrep "any $word where word!=html and word!=form"
16:11  pgas: you'll need something a bit more clever than grep, like awk
16:11  xelister: yeah or awk, any tool
16:11  xelister: so... how to? :)
16:14  xelister: anyone?
16:14  pgas: arg...
16:14  pgas: awk '!/[$]/{next} /[$]html/{m++} /[$]form/{m++} m==1{c++}{m=0} END{print c}'
16:15  pgas: something like that
16:15  xelister: hm
16:17  xelister: pgas: that pints count of lines with $html and $form it seems
16:23  pgas: 4# echo '$form $html' |  awk '!/[$]/{next} /[$]html/{m++} /[$]form/{m++} m==1{c++}{m=0}END{print c+0}'
16:23  shbot: pgas: 0
16:30  arturas: xelister: echo -e '$x\n$html\n$html $foo' |  awk '$0!="$html" && $0!="$form"{c++} END{print c}'
16:36  xelister: arturas: that count all words that are not html or foo, instead of all  "$words" like  $foo.  There is a literal dolar sign there
16:37  xelister: literal dolar sign
16:43  arturas: find . -type f | xargs awk '$0!="$html" && $0!="$form" && $0~/^\$/{c++} END{print c}'
16:44  arturas: so you need lines that starts with '$' and are not '$html', '$form', etc?
16:45  arturas: or how many lines have words that starts with '$' and some particular word, wich is not 'html', 'form'
16:46  xelister: ok let me rephrase
16:46  xelister: Count all lines that contain words that are not "html" nor "form" and are starting with literal dolar.
16:46  xelister: or even better
16:46  xelister: Count all lines that contain words that are not "$html" nor "$form" and are starting with literal dolar.
--- Log closed Mon Dec 20 16:50:57 2010
--- Log opened Mon Dec 20 16:51:41 2010
16:51 --- Users 94 nicks [1 ops, 0 halfops, 0 voices, 93 normal]
16:53 --- Channel #awk was synced in 104 seconds
16:54  arturas: http://eugeneciurana.com/pastebin/pastebin.php?show=43680
16:59  steve___: xelister: like this -- awk '!/\$html/ && !/\$form/ && /\$.*/' foo/* | wc -l
17:31  probonono: xelister, you can use awk to filter out the unwanted lines with something like:  awk '{gsub("[$](html|form)", "")} /[$][[:alpha:]]+/'
17:31  xelister: I finally used   | sed -e 's/\$html//;s/\$form//'|grep -c -E '\$\w+'
18:00  sente: pgas: how is [$] interpretted?
18:00  sente: i've never seen that
18:12  probonono: sente, it's just a "bracket" expression, like:  [0-9]  or  [,.:;+=$]  etc, but just containing the single character "$". It simplifies the shell commands by avoiding messy backslash escaping of the "$".
18:14  sente: ahh. makes sense. thank you, probonono 
18:15  probonono: you're very welcome
--- Log closed Mon Dec 20 21:34:39 2010
--- Log opened Mon Dec 20 21:40:01 2010
21:40 --- Users 94 nicks [1 ops, 0 halfops, 0 voices, 93 normal]
21:41 --- Channel #awk was synced in 103 seconds
22:29 --- xelister is now known as xelister_
--- Log closed Mon Dec 20 23:31:37 2010
--- Log opened Mon Dec 20 23:31:58 2010
23:31 --- Users 94 nicks [1 ops, 0 halfops, 0 voices, 93 normal]
23:33 --- Channel #awk was synced in 103 seconds
--- Log closed Tue Dec 21 00:00:37 2010
